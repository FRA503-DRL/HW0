{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 1: Take a Look at Cartpole Rl Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. According to the tutorials, if we want to edit the environment configuration, action space, observation space, reward function, or termination condition of the Isaac-Cartpole-v0 task, which file should we look at, and where is each part located?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source code:** https://github.com/isaac-sim/IsaacLab/blob/main/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/classic/cartpole/cartpole_env_cfg.py\n",
    "\n",
    "We should look at cartpole_env_cfg.py that located in Home/IsaacLab/blob/main/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/classic/cartpole/cartpole_env_cfg.py and each part located\n",
    "\n",
    "**Environment configuration** in line 157-181\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@configclass\n",
    "class CartpoleEnvCfg(ManagerBasedRLEnvCfg):\n",
    "    \"\"\"Configuration for the cartpole environment.\"\"\"\n",
    "\n",
    "    # Scene settings\n",
    "    scene: CartpoleSceneCfg = CartpoleSceneCfg(num_envs=4096, env_spacing=4.0)\n",
    "    # Basic settings\n",
    "    observations: ObservationsCfg = ObservationsCfg()\n",
    "    actions: ActionsCfg = ActionsCfg()\n",
    "    events: EventCfg = EventCfg()\n",
    "    # MDP settings\n",
    "    rewards: RewardsCfg = RewardsCfg()\n",
    "    terminations: TerminationsCfg = TerminationsCfg()\n",
    "\n",
    "    # Post initialization\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Post initialization.\"\"\"\n",
    "        # general settings\n",
    "        self.decimation = 2\n",
    "        self.episode_length_s = 5\n",
    "        # viewer settings\n",
    "        self.viewer.eye = (8.0, 0.0, 5.0)\n",
    "        # simulation settings\n",
    "        self.sim.dt = 1 / 120\n",
    "        self.sim.render_interval = self.decimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action space** in line 58-62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@configclass\n",
    "class ActionsCfg:\n",
    "    \"\"\"Action specifications for the MDP.\"\"\"\n",
    "\n",
    "    joint_effort = mdp.JointEffortActionCfg(asset_name=\"robot\", joint_names=[\"slider_to_cart\"], scale=100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation space** in line 65-85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@configclass\n",
    "class ObservationsCfg:\n",
    "    \"\"\"Observation specifications for the MDP.\"\"\"\n",
    "\n",
    "    @configclass\n",
    "    class PolicyCfg(ObsGroup):\n",
    "        \"\"\"Observations for policy group.\"\"\"\n",
    "\n",
    "        # observation terms (order preserved)\n",
    "        joint_pos_rel = ObsTerm(func=mdp.joint_pos_rel)\n",
    "        joint_vel_rel = ObsTerm(func=mdp.joint_vel_rel)\n",
    "\n",
    "        def __post_init__(self) -> None:\n",
    "            self.enable_corruption = False\n",
    "            self.concatenate_terms = True\n",
    "\n",
    "    # observation groups\n",
    "    policy: PolicyCfg = PolicyCfg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reward function** in line 111-136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@configclass\n",
    "class RewardsCfg:\n",
    "    \"\"\"Reward terms for the MDP.\"\"\"\n",
    "\n",
    "    # (1) Constant running reward\n",
    "    alive = RewTerm(func=mdp.is_alive, weight=1.0)\n",
    "    # (2) Failure penalty\n",
    "    terminating = RewTerm(func=mdp.is_terminated, weight=-2.0)\n",
    "    # (3) Primary task: keep pole upright\n",
    "    pole_pos = RewTerm(\n",
    "        func=mdp.joint_pos_target_l2,\n",
    "        weight=-1.0,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"]), \"target\": 0.0},\n",
    "    )\n",
    "    # (4) Shaping tasks: lower cart velocity\n",
    "    cart_vel = RewTerm(\n",
    "        func=mdp.joint_vel_l1,\n",
    "        weight=-0.01,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"slider_to_cart\"])},\n",
    "    )\n",
    "    # (5) Shaping tasks: lower pole angular velocity\n",
    "    pole_vel = RewTerm(\n",
    "        func=mdp.joint_vel_l1,\n",
    "        weight=-0.005,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"])},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Termination condition** in line 139-149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@configclass\n",
    "class TerminationsCfg:\n",
    "    \"\"\"Termination terms for the MDP.\"\"\"\n",
    "\n",
    "    # (1) Time out\n",
    "    time_out = DoneTerm(func=mdp.time_out, time_out=True)\n",
    "    # (2) Cart out of bounds\n",
    "    cart_out_of_bounds = DoneTerm(\n",
    "        func=mdp.joint_pos_out_of_manual_limit,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"slider_to_cart\"]), \"bounds\": (-3.0, 3.0)},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the action space and observation space for an agent defined in the Isaac-Cartpole-v0 task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space is action controls the torque applied to the joint which moves the cart horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@configclass\n",
    "class ActionsCfg:\n",
    "    \"\"\"Action specifications for the MDP.\"\"\"\n",
    "\n",
    "    joint_effort = mdp.JointEffortActionCfg(asset_name=\"robot\", joint_names=[\"slider_to_cart\"], scale=100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation space includes:\n",
    "1. Relative Position (joint_pos_rel):\n",
    "- Cart Position: Horizontal position of the cart.\n",
    "- Pole Angle: Angular position of the pole relative to the vertical.\n",
    "2. Relative Velocity (joint_vel_rel):\n",
    "- Cart Velocity: Horizontal velocity of the cart.\n",
    "- Pole Angular Velocity: Angular velocity of the pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@configclass\n",
    "class ObservationsCfg:\n",
    "    \"\"\"Observation specifications for the MDP.\"\"\"\n",
    "\n",
    "    @configclass\n",
    "    class PolicyCfg(ObsGroup):\n",
    "        \"\"\"Observations for policy group.\"\"\"\n",
    "\n",
    "        # observation terms (order preserved)\n",
    "        joint_pos_rel = ObsTerm(func=mdp.joint_pos_rel)\n",
    "        joint_vel_rel = ObsTerm(func=mdp.joint_vel_rel)\n",
    "\n",
    "        def __post_init__(self) -> None:\n",
    "            self.enable_corruption = False\n",
    "            self.concatenate_terms = True\n",
    "\n",
    "    # observation groups\n",
    "    policy: PolicyCfg = PolicyCfg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. How can episodes in the Isaac-Cartpole-v0 task be terminated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Isaac-Cartpole-v0 task, episodes can be terminated in two ways as\n",
    "\n",
    "1. Time out: The episode terminates when a predefined time limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "time_out = DoneTerm(func=mdp.time_out, time_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cart out of bounds: The episode terminates if the cart moves beyond specified position limits, which checks whether the cart's position lies outside the bounds (-3.0, 3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cart_out_of_bounds = DoneTerm(\n",
    "        func=mdp.joint_pos_out_of_manual_limit,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"slider_to_cart\"]), \"bounds\": (-3.0, 3.0)},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. How many reward terms are used to train an agent in the Isaac-Cartpole-v0 task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 reward terms line 111-136\n",
    "\n",
    "1. Constant running reward เป็น Reward (+1) ที่ให้เมื่อ Agent ยังสามารถมีชีวิตรอดได้ในแต่ละ Step เพื่อสนับสนุนให้ Agent พยายามอยู่รอดในเกมให้ได้นานที่สุด\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "alive = RewTerm(func=mdp.is_alive, weight=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Failure penalty เป็น Reward ลดแต้ม (-2) เมื่อ Cartpole ออกนอกขอบเขต เพื่อกระตุ้นให้ Agent หลีกเลี่ยงการเข้าสู่สถานะล้มเหลว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " terminating = RewTerm(func=mdp.is_terminated, weight=-2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Primary task: keep pole upright เป็นการกระตุ้นให้เสาตั้งตรง หากเสามีการเอียงจากตำแหน่งตั้งตรง (Target Position) จะได้บทลงโทษ เป็น Reward ติดลบมากขึ้นตามระยะห่างระหว่าง Pole Position กับ Target Position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pole_pos = RewTerm(\n",
    "        func=mdp.joint_pos_target_l2,\n",
    "        weight=-1.0,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"]), \"target\": 0.0},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. Shaping tasks: lower cart velocity เป็น Reward ที่สนับสนุนให้ Agent ลด Cart Velocity เพื่อเพิ่มความเสถียรให้กับระบบ ให้ Agent เรียนรู้ที่จะเคลื่อนที่ด้วยความเร็วที่เหมาะสม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cart_vel = RewTerm(\n",
    "        func=mdp.joint_vel_l1,\n",
    "        weight=-0.01,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"slider_to_cart\"])},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Shaping tasks: lower pole angular velocity เป็น Reward ที่ลด Pole Angular Velocity เพื่อเพิ่มความเสถียรให้กับระบบ ป้องกันไม่ให้เสาแกว่งมากเกินไปจนเสียสมดุล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    pole_vel = RewTerm(\n",
    "        func=mdp.joint_vel_l1,\n",
    "        weight=-0.005,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"])},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 2: Playing with Cartpole Rl Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us adjust the weight of each reward term specified in the Isaac-Cartpole-v0 task and train the agent. Which results will be affected by this adjustment, and why? Submit the answers.\n",
    "\n",
    "You may further explore by modifying other aspects, such as the agent's action space, observation space, and termination conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 3: Mapping RL Fundamentals**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is reinforcement learning and its components according to your understanding? Giving examples of each component according to the diagram consider the Cartpole problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning คือ แนวทางการเรียนรู้ด้วยการที่ Agent จะทำ Action ใดก็ได้เพื่อให้ได้ Reward มากที่สุด เพื่อเพิ่ม Value Function ให้ได้มากที่สุด และในระหว่างทางก็จะมีการเรียนรู้เกิดขึ้นว่า Action ไหนดีหรือไม่ดี\n",
    "\n",
    "**1. Agent:** ตัวแทนที่จะคอยเรียนรู้และตัดสินใจ ดูว่าใน State นี้ ควรจะมี Action อย่างไร\n",
    "- Cartpole problem: การเรียนรู้ที่จะทำให้ Pole ตั้งตรง ไม่เอนเอียงไปมา\n",
    "\n",
    "**2. Environment:** ระบบที่ Agent ต้องมีการตอบโต้ด้วย\n",
    "- Cartpole problem: การจำลองฟิสิกส์สำหรับการเคลื่อนที่ของ Cartpole\n",
    "\n",
    "**3. State:** สถานการณ์ในเวลาหนึ่ง\n",
    "- Cartpole problem: Torque (ไม่ชัวร์), Cart Position, Pole Angle, Cart Velocity, Pole Angular Velocity\n",
    "\n",
    "**4. Reward:** สิ่งที่ใช้ในการประเมินผลลัพธ์ที่จะเกิดจากการกระทำของ Agent ว่าดีหรือไม่ดี เพื่อบอกว่า Agent ควรจะเลือกทำ Action ไหน\n",
    "- Cartpole problem:\n",
    "    - Reward +1 เมื่อ Agent ยังมีชีวิตรอดอยู่\n",
    "    - Reward -2 เมื่อ Cartpole ออกนอกขอบเขตแล้ว Agent เข้าสู่สถานะล้มเหลว\n",
    "    - โดนลด Reward ตามระยะห่างระหว่าง Pole Position กับ Target Position\n",
    "    - โดนลด Reward หาก Cart Velocity ไม่เหมาะสม\n",
    "    - โดนลด Reward หาก Pole Angular Velocity ไม่เหมาะสม\n",
    "\n",
    "**5. Action:** การกระทำ\n",
    "- Cartpole problem: การกำหนด Torque, Cart Position, Pole Angle, Cart Velocity, Pole Angular Velocity\n",
    "\n",
    "**6. Policy:** หลักการที่ Agent ใช้ในการตัดสินใจหลังจากวิเคราะห์ความเป็นไปได้ที่เกิดจากการได้รับ Reward หลังทำ Action ก่อนหน้า\n",
    "- Cartpole problem: ฟังก์ชันที่จับคู่ State กับความน่าจะเป็นของ Action (เช่น ซ้ายหรือขวา)\n",
    "\n",
    "**7. Value Function:** การประเมิน Reward สะสมที่คาดหวังของ State ปัจจุบัน หลังจากทำตาม Policy\n",
    "- Cartpole problem: การคาดการณ์รางวัลสะสมจาก Reward ที่ได้รับในแต่ละ case\n",
    "\n",
    "**8. Model:**\n",
    "- Cartpole problem:\n",
    "\n",
    "**9 History:** ข้อมูลในอดีตที่ Agent เก็บไว้เพื่อการเรียนรู้ เช่น การเก็บลำดับของ State, Action, และ Reward (คิดว่าไม่ใช่แต่แปะไว้ก่องงง)\n",
    "- Cartpole problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is the difference between the reward, return, and the value function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reward:** ผลตอบแทนที่ได้รับทันทีหลังจากมี Action ใน State ปัจจุบัน\n",
    "\n",
    "**Return:** ผลตอบแทนสะสมในอนาคต ที่เริ่มต้นจากเวลาปัจจุบัน(t)\n",
    "\n",
    "**Value Function:** การคาดการณ์ Return เฉลี่ยในอนาคตจาก State หนึ่ง โดยขึ้นอยู่กับ Policy ที่ใช้อยู่"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Consider policy, state, value function, and model as mathematical functions, what would each one take as input and output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy**\n",
    "- input: State\n",
    "- output: ความน่าจะเป็นของ Action\n",
    "\n",
    "**State**\n",
    "- input: -\n",
    "- output: การอธิบายลักษณะของ Environment ในเวลานั้น เช่น ความเร็ว\n",
    "\n",
    "**Value Function**\n",
    "- input: State\n",
    "- output: Reward สะสมที่คาดหวังเมื่อเริ่มต้นจาก State นั้นและปฏิบัติตาม Policy\n",
    "\n",
    "**Model**\n",
    "- input: State และ Action\n",
    "- output: ความน่าจะเป็นของการเปลี่ยนแปลงไปยัง State ถัดไป"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
